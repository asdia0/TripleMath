\chapter{Inequalities}

\section{Triangle Inequality}

\begin{theorem}[Triangle Inequality]
    For all $a, b \in \RR$, we have \[\abs{a + b} \leq \abs{a} + \abs{b}.\] Equality occurs if and only if $a$ and $b$ have the same sign.
\end{theorem}
\begin{proof}
    We have \[-\abs{a} \leq a \leq a \quad \tand \quad -\abs{b} \leq b \leq b.\] Adding both inequalities, we get \[-\bp{\abs{a} + \abs{b}} \leq a + b \leq \abs{a} + \abs{b},\] so $\abs{a + b} \leq \abs{a} + \abs{b}$ as desired.
\end{proof}

The triangle inequality gets its name from the fact that the sum of any two sides in a triangle is always larger than the remaining side. In general, given a vector space $V$, the vectors $\vec u$, $\vec v$ and $\vec u + \vec v$ form the sides of a triangle, so \[\abs{\vec u + \vec v} \leq \abs{\vec u} + \abs{\vec v}.\]

The triangle inequality can be used to give bounds on series and integrals.

\begin{corollary}
    For all sequences $\bc{a_i}$, we have \[\abs{\sum_{i = 1}^n a_i} \leq \sum_{i = 1}^n \abs{a_i}.\] Equality occurs if and only if all terms have the same sign.
\end{corollary}
\begin{proof}
    Induct on $n$.
\end{proof}

\begin{corollary}
    Suppose $f$ is integrable over $D$. Then \[\abs{\int_D f(x) \d x} \leq \int_D \abs{f(x)} \d x.\] Equality occurs if and only if $f(x)$ is non-negative or non-negative almost everywhere.
\end{corollary}

\section{Jensen's Inequality}

Recall that a function $f$ is said to be convex on an interval $I$ if for any two points $x_1, x_2 \in I$ and weights $t_1, t_2 > 0$ with $t_1 + t_2 = 1$, we have \[f(t_1 x_1 + t_2 x_2) \leq t_1 f(x_1) + t_2 f(x_2).\] We can easily generalize this result to the case with $n$ points and $n$ weights.

\begin{theorem}[Jensen's Inequality (Convex)]
    Let $f$ be convex on $I$. Given $x_i \in I$ and $t_i > 0$ for $i = 1, \dots, n$ with $\sum_{t = 1}^n t_i = 1$, we have \[f\!\bp{\sum_{i = 1}^n t_i x_i} \leq \sum_{i = 1}^n t_i f(x_i).\] Equality occurs if and only if $x_1 = \dots = x_n$ or $f$ is linear on the convex hull of $\bc{x_i}$.
\end{theorem}
\begin{proof}
    We induct on $n$. Note that the $n = 1$ is trivial and the $n = 2$ case is true by definition. Now suppose our result holds for some $n \in \NN$. Then
    \begin{align*}
        f\!\bp{\sum_{i = 1}^{n+1} t_i x_i} &= f\!\bp{t_{n+1}x_{n+1} + (1-t_{n+1}) \sum_{i = 1}^n \frac{t_i}{1 - t_{n+1}} x_i}\\
        &\leq t_{n+1} f(x_{n+1}) + (1-t_{n+1}) f\!\bp{\sum_{i = 1}^n \frac{t_i}{1 - t_{n+1}} x_i}
    \end{align*}
    where we used the $n = 2$ case in the second line. Since \[\sum_{i = 1}^n \frac{t_i}{1 - t_{n+1}} = \frac{1 - t_{n+1}}{1 - t_{n+1}} = 1,\] by our induction hypothesis, we have
    \begin{align*}
        t_{n+1} f(x_{n+1}) + (1-t_{n+1}) f\!\bp{\sum_{i = 1}^n \frac{t_i}{1 - t_{n+1}} x_i} &\leq t_{n+1} x_{n+1} + (1-t_{n+1}) \sum_{i = 1}^n \frac{t_i}{1 - t_{n+1}} f(x_i)\\
        &= \sum_{i = 1}^{n+1} t_i f(x_i).
    \end{align*}
    This closes the induction and we are done.
\end{proof}

If $f$ is concave, we get an analogous result.

\begin{theorem}[Jensen's Inequality (Concave)]
    Let $f$ be concave on $I$. Given $x_i \in I$ and $t_i > 0$ for $i = 1, \dots, n$ with $\sum_{t = 1}^n t_i = 1$, we have \[f\!\bp{\sum_{i = 1}^n t_i x_i} \geq \sum_{i = 1}^n t_i f(x_i).\] Equality occurs if and only if $x_1 = \dots = x_n$ or $f$ is linear on the convex hull of $\bc{x_n}$.
\end{theorem}

\section{AM-GM Inequality}

\begin{definition}
    For real numbers $x_1, \dots, x_n$, the \vocab{arithmetic mean} is defined as $\sum_{i = 1}^n \frac{x_n}{n}$ and the \vocab{geometric mean} is defined as $\prod_{i = 1}^n x_i^{1/n}$.
\end{definition}

\begin{theorem}[AM-GM Inequality]
    For non-negative real numbers $x_1, \dots, x_n$, the arithmetic mean is greater than or equal to the geometric mean: \[\frac{x_1 + \dots x_n}{n} \geq (x_1 \dots x_n)^{1/n}.\] Equality holds if and only if $x_1 = \dots = x_n$.
\end{theorem}
\begin{proof}
    Since the logarithm function is concave, by Jensen's inequality, we have \[\ln{\sum_{i = 1}^n \frac{x_i}n} \geq \sum_{i = 1}^n \frac{\ln{x_i}}n = \frac1n \ln{\prod_{i = 1}^n x_i} = \ln{\prod_{i = 1}^n x_i^{1/n}}.\] Since exponentiation is monotonic, we have \[\frac{x_1 + \dots x_n}{n} \geq (x_1 \dots x_n)^{1/n},\] which is precisely the AM-GM inequality.
\end{proof}

\begin{sample}
    If $a$, $b$ and $c$ are positive, prove that $(a+b)(b+c)(c+a) \geq 8abc$.
\end{sample}
\begin{sampans}
    By the AM-GM inequality, we have \[a + b \geq 2\sqrt{ab}, \quad b + c \geq 2\sqrt{bc}, \quad c + a \geq 2\sqrt{ca}.\] Multiplying these three inequalities together, we obtain \[(a+b)(b+c)(c+a) \geq \bp{2\sqrt{ab}}\bp{2\sqrt{bc}}\bp{2\sqrt{ca}} \geq 8abc\] as desired.
\end{sampans}

\section{Cauchy-Schwarz Inequality}

The general form of the Cauchy-Schwarz inequality relates the magnitude of an inner product to the product of norms.

\begin{theorem}[Cauchy-Schwarz Inequality]
    Let $V$ be a vector space with an inner product $\ba{\cdot, \cdot}$. Then for any vectors $\vec u, \vec v \in V$, we have \[\abs{\ba{\vec u, \vec v}}^2 \leq \norm{\vec u} \norm{\vec v},\] where $\norm{\cdot}$ is the norm induced by the inner product. Equality holds if and only if $\vec u$ and $\vec v$ are linearly dependent.
\end{theorem}

The more familiar Cauchy-Schwarz inequality is recovered when $V$ is the Euclidean $n$-space $\RR^n$ with the dot product as the inner product.

\begin{proposition}[Cauchy-Schwarz Inequality ($\RR^n$)]
    For real numbers $a_1, \dots, a_n$ and $b_1, \dots, b_n$, \[\bp{\sum_{i = 1}^n a_i^2} \bp{\sum_{i = 1}^n b_i^2} \geq \bp{\sum_{i =1}^n a_i b_i}^2\] with equality if and only if $a_i = 0$, $b_i = 0$, or $a_i = \l b_i$ for some constant $\l$.
\end{proposition}
\begin{proof}[Proof 1 (Dot Product).]
    For any two vectors $\vec a, \vec b \in \RR^n$, we have \[\vec a \dotp \vec b = \abs{\vec a} \abs{\vec b} \cos \t \leq \abs{\vec a} \abs{\vec b}.\] Letting $\vec a = \cvecivx{a_1}{a_2}{\dots}{a_n}$ and $\vec b = \cvecivx{b_1}{b_2}{\dots}{b_n}$, we get the Cauchy-Schwarz inequality. Equality holds when the two vectors are parallel, i.e. $a_i = \l b_i$ for constant $\l$, or when either vector is the zero vector.
\end{proof}
\begin{proof}[Proof 2 (Discriminant).]
    Let $t$ be an arbitrary real number. Observe that $\sum (a_i t - b_i)^2$ is non-negative. Expanding, we get a quadratic in $t$: \[\sum_{i = 1}^n (a_i t - b)^2 = \bp{\sum_{i = 1}^n a_i^2} t^2 - \bp{2\sum_{i = 1}^n a_i b_i} t + \sum_{i = 1}^n b^2.\] For at most one root, the discriminant must thus be non-positive, so \[\bp{2\sum_{i = 1}^n a_i b_i}^2 - 4\bp{\sum_{i = 1}^n a_i^2}\bp{\sum_{i = 1}^n b_i^2} \leq 0.\] Rearranging, we get the Cauchy-Schwarz inequality.
    
    If $\sum a_i^2 = 0$ or $\sum b_i^2 = 0$, then equality trivially holds. Else, $t \neq 0$ and equality holds when $\sum (a_i t - b_i)^2 = 0$, i.e. $a_i = (1/t) b_i$ for all $1 \leq i \leq n$.
\end{proof}

\begin{sample}
    Prove Titu's Lemma: \[\frac{x_1^2}{y_1} + \dots + \frac{x_n^2}{y_n} \geq \frac{(x_1 + \dots + x_n)^2}{y_1 + \dots + y_n}\] for all positive real numbers $y_1, \dots, y_n$.
\end{sample}
\begin{sampans}
    Let $a_i = x_i/\sqrt{y_i}$ and $b_i = \sqrt{y_i}$. By the Cauchy-Schwarz inequality, \[\bp{\frac{x_1^2}{y_1} + \dots + \frac{x_n^2}{y_n}}\bp{y_1 + \dots + y_n} \geq \bp{x_1 + \dots + x_n}^2\] and we immediately acquire the desired result.
\end{sampans}

The general triangle inequality is a simple corollary of the Cauchy-Schwarz inequality in $\RR^n$.

\begin{corollary}[Triangle Inequalty in $\RR^n$]
    For $\vec u, \vec v \in \RR^n$, we have $\abs{\vec u + \vec v} \leq \abs{\vec u} + \abs{\vec v}$.
\end{corollary}
\begin{proof}
    Let $\vec u, \vec v \in \RR^n$ with inner product $\ba{\vec u, \vec v} = \vec u \dotp \vec v$. Then
    \begin{align*}
        \abs{\vec u + \vec v}^2 &= \abs{\vec u}^2 + 2(\vec u \dotp \vec v) + \abs{\vec v}^2\\
        &\leq \abs{\vec u}^2 + 2\abs{\vec u \dotp \vec v} + \abs{\vec v}^2\\
        &\leq \abs{\vec u}^2 + 2\abs{\vec u}\abs{\vec v} + \abs{\vec v}^2\\
        &\leq \bp{\abs{\vec u} + \abs{\vec v}}^2
    \end{align*}
    Taking roots, we get our desired result.
\end{proof}