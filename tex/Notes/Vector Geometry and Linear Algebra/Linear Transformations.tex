\chapter{Linear Transformations}

\begin{definition}
    A \vocab{linear transformation} is a function $T : \RR^n \to \RR^m$ is a function that satisfies the following two properties:
    \begin{itemize}
        \item $T(\vec u + \vec v) = T(\vec u) + T(\vec v)$ for all vectors $\vec u, \vec v \in \RR^n$,
        \item $T(k \vec u) = k T(\vec u)$ for all scalars $k \in \RR$ and vectors $\vec u \in \RR^n$.
    \end{itemize}
\end{definition}

Taken together, these two properties mean that linear transformations preserve the structure of linear combinations.

\begin{proposition}[Linear Transformations Preserve Linear Combinations]
    Let $k_1, \dots, k_r \in \RR$ and $\vec v_1, \dots, \vec v_r \in \RR^n$. Then $T(k_1 \vec v_1 + \dots + k_r \vec v_r) = k_1 T(\vec v_1) + \dots + k_r T(\vec v_r)$.
\end{proposition}

When $k = 0$, the second property of linear transformations also implies that $T(\vec 0) = \vec 0$. That is, a linear transformation must map $\vec 0$ to $\vec 0$.

\begin{recipe}[Determining if a Function is a Linear Transformation]
    To determine if a function $f : \RR^n \to \RR^m$ is a linear transformation, we go through the following ``checklist'', arranged in increasing difficulty to see:
    \begin{itemize}
        \item Check if $f(\vec 0) = \vec 0$.
        \item Check if $f(k \vec v) = k f(\vec v)$.
        \item Check if $f(\vec u + \vec v) = f(\vec u) + f(\vec v)$.
    \end{itemize}
    If $f$ passes the above checklist, we then proceed to show that $f(k_1 \vec v_1 + k_2 \vec v_2) = k_1 f(\vec v_1) + k_2 f(\vec v_2)$. This would immediately imply that $f$ satisfies the two properties and is thus a linear transformation.
\end{recipe}

\begin{sample}
    Let $T: \RR^2 \to \RR^3$ be a function defined by \[T\!\cvecii{x}{y} = \cveciii{x}{x+y}{x-y}.\] Determine if $T$ is a linear transformation. 
\end{sample}
Clearly, $T(\vec 0) = \vec 0$, so $T$ passes the first check. By inspection, $T$ also satisfies the remaining two checks. We are now confident that $T$ is a linear transformation, so we proceed with a formal proof.
\begin{sampans}
    Consider $T(k_1 \vec v_1 + k_2 \vec v_2)$, where $\vec v_1 = \cveciix{x_1}{y_1}$ and $\vec v_2 = \cveciix{x_2}{y_2}$. Then
    \begin{align*}
        T(k_1 \vec v_1 + k_2 \vec v_2) &= T\!\cvecii{k_1 x_1 + k_2 x_2}{k_1 y_1 + k_2 y_2} = \cveciii{k_1 x_1 + k_2 x_2}{\bp{k_1 x_1 + k_2 x_2} + \bp{k_1 y_1 + k_2 y_2}}{\bp{k_1 x_1 + k_2 x_2} - \bp{k_1 y_1 + k_2 y_2}}\\
        &= k_1\cveciii{x_1}{x_1 + y_1}{x_1 - y_1} + k_2 \cveciii{x_2}{x_2 + y_2}{x_2 - y_2}\\
        &= k_1 T(\vec v_1) + k_2 T(\vec v_2).
    \end{align*}
    Thus, $T$ is indeed a linear transformation.
\end{sampans}

\section{Matrix Representation}

Observe that the transformation $T$ in the above example may also be written as \[T\!\cvecii{x}{y} = \cveciii{x}{x+y}{x-y} = \begin{pmatrix}1 & 0 \\ 1 & 1 \\ 1 & -1\end{pmatrix} \cvecii{x}{y}.\] This is because matrix multiplication may also be seen as a form of linear transformation.

\begin{proposition}[Matrix Multiplication is a Linear Transformation]
    Let $\mat A$ be an $m \times n$ matrix. Then, multiplication by $\mat A$ will take an $n$-dimensional vector to an $m$-dimensional vector, so $T(\vec x) = \mat A \vec x$ is a function from $\RR^n$ to $\RR^m$. Moreover, it is linear, as for any $\vec x, \vec y \in \RR^n$ and $k \in \RR$, we have $T(\vec x + \vec y) = \mat A (\vec x + \vec y) = \mat A \vec x + \mat A \vec y = T(\vec x) + T(\vec y)$ and $T(k\vec x) = \mat A (k \vec x) = k \mat A \vec x = k T(\vec x)$.
\end{proposition}

Surprisingly, there are no other examples of linear transformations from $\RR^n$ to $\RR^m$; matrix multiplication is the only kind of linear transformation there is for functions between finite-dimensional spaces:

\begin{proposition}
    Let $T : \RR^n \to \RR^m$ be a linear transformation. Then there exists a unique $m \times n$ matrix $\mat A$ such that $T(\vec x) = \mat A \vec x$ for all $\vec x \in \RR^n$.
\end{proposition}
\begin{proof}
    Let $\vec e_i$ be the $i$th standard basis vector. Let $\vec x = (x_1, \dots, x_n)$ be an $n$-dimensional vector.
    
    \item\bd{Existence.} We have
    \begin{align*}
        T(\vec x) &= T(x_1 \vec e_1 + \dots + x_n \vec e_n)\\
        &= x_1 T(\vec e_1) + \dots + x_n T(\vec e_n)\\
        &= \underbrace{\begin{pmatrix}T(\vec e_1) & \cdots & T(\vec e_n)\end{pmatrix}}_{\mat A} \vec x.
    \end{align*}
    Since $T(\vec e_i)$ is an $m$-dimensional vector (by the definition of $T$), it follows that $\mat A$ has $m$ rows and $n$ columns, i.e. $\mat A$ is an $m \times n$ matrix.

    \item\bd{Uniqueness.} Let $\mat A$ and $\mat B$ be $m \times n$ matrices such that $T(\vec x) = \mat A \vec x = \mat B \vec x$ for all $\vec x \in \RR^n$. Then $\bp{\mat A - \mat B} \vec x = \vec 0$ for all $\vec x \in \RR^n$. In particular, for $\vec x = \vec e_i$, we have $\bp{\mat A - \mat B} \vec e_i = \vec 0$. But $\bp{\mat A - \mat B} \vec e_i$ is the $i$th column of $\mat A - \mat B$, so $\mat A - \mat B$ must be the zero matrix, whence $\mat A = \mat B$ and the matrix representation of $T$ is unique.
\end{proof}

\begin{sample}
    Let $T : \RR^2 \to \RR^3$ be a linear transformation such that $T\!\cveciix12 = \cveciiix357$ and $T\!\cveciix31 = \cveciiix202$. Find a matrix $\mat A$ such that $T(\vec x) = \mat A \vec x$ for all $\vec x \in \RR^2$.
\end{sample}
\begin{solution}
    Note that \[\cvecii10 = -\frac15 \cvecii12 + \frac25 \cvecii31 \quad \tand \quad \cvecii01 = \frac35 \cvecii12 - \frac15 \cvecii31.\] Since $T$ is a linear transformation, we have
    \begin{align*}
        T\!\cvecii10 &= -\frac15 \, T\!\cvecii12 + \frac25 \, T\!\cvecii31 = \frac15 \cveciii{1}{-5}{-3},\\
        T\!\cvecii01 &= \frac35 \, T\!\cvecii12 - \frac15 \, T\!\cvecii31 = \frac15 \cveciii7{15}{19},
    \end{align*}
    so \[T(\vec x) = \frac 15 \begin{pmatrix}1 & 7 \\ -5 & 15 \\ -3 & 19\end{pmatrix} \vec x\] for all $\vec x \in \RR^2$.
\end{solution}

\section{Linear Spaces}

\begin{definition}
    A \vocab{linear space} (or \vocab{vector space}) over $\RR$ is a set $V$ equipped with two operations, addition ($+$) and scalar multiplication ($\cdot$), such that for any vectors $\vec u, \vec v, \vec w \in V$ and for all $c, d \in \RR$, the following ten axioms are satisfied:
    \renewcommand{\theenumi}{\arabic{enumi}.}
    \begin{enumerate}
        \item Closure under addition: $\vec u + \vec v \in V$.
        \item Addition is commutative: $\vec u + \vec v = \vec v + \vec u$.
        \item Addition is associative: $(\vec u + \vec v) + \vec w = \vec u + (\vec v + \vec w)$.
        \item Existence of additive identity: There is a zero vector, $\vec 0$, such that $\vec 0 + \vec u = \vec u$.
        \item Existence of additive inverse: There is a vector $-\vec u$ such that $\vec u + (-\vec u) = \vec 0$.
        \item Closure under scalar multiplication: $c\vec u \in V$.
        \item Scalar multiplication is distributive over vector addition: $c(\vec u + \vec v) = c \vec u + c \vec v$.
        \item Scalar multiplication is distributive over scalar addition: $(c + d) \vec u = c \vec u + d \vec u$.
        \item Scalar multiplication is associative: $c(d \vec u) = (cd) \vec u$.
        \item Existence of scalar multiplicative identity: There exists a scalar, 1, such that $1 \vec u = \vec u$.
    \end{enumerate}
    \renewcommand{\theenumi}{(\alph{enumi})}
\end{definition}

One can think of a linear space as an Abelian group (under addition, Axioms 1-5) with the added structure of ``scalar multiplication'' (Axioms 6-10).

\subsection{Examples of Linear Spaces}

\begin{definition}
    The \vocab{Euclidean $n$-space}, $\RR^n = \bc{(u_1, \dots, u_n) \mid u_1, \dots, u_n \in \RR}$, is the set of all $n$-vectors (ordered $n$-tuples) of real numbers.
\end{definition}

\begin{proposition}
    $\RR^n$ is a linear space equipped with scalar addition and scalar multiplication.
\end{proposition}

$\RR^n$ is the quintessential example of a linear space, and is the linear space that we will deal with most. We can also generalize the above statements from vectors to matrices.

\begin{proposition}
    The set of all $m \times n$ matrices with real entries forms a linear space (equipped with matrix addition and scalar multiplication).
\end{proposition}

There are also more abstract examples of linear spaces.

\begin{proposition}
    The set of all polynomials with real coefficients of at most degree $n \geq 0$, forms a linear space under the usual addition and multiplication operations.
\end{proposition}

Lastly, there is the zero (or trivial) vector space.

\begin{definition}
    Let $V$ be a singleton, i.e. $V = \bc{\vec 0}$. Define $\vec 0 + \vec 0 = \vec 0$ and $k \vec 0 = \vec 0$ for all scalars $k$. Then $V$ is the \vocab{zero vector space}.
\end{definition}

\section{Subspaces}

\begin{definition}
    Suppose $V$ is a linear space under $(+, \dotp)$, and $W \subseteq V$. If $W$ is also a linear space under $(+, \dotp)$, then $W$ is a \vocab{subspace} of $V$.
\end{definition}

Because subspaces inherit addition and multiplication, we do not need to check Axioms 2, 3, 7, 8 and 9. Further, Axiom 5 is guaranteed if Axiom 6 is valid. Thus, we only need to verify Axioms 1, 4 and 6 when testing for subspaces.

\begin{recipe}[Test for Subspace]
    Let $W$ be a non-empty subset of a linear space $V$. Then $W$ is a subspace of $V$ if and only if the following conditions hold:
    \begin{itemize}
        \item $\vec 0 \in W$.
        \item (Closure under addition) For all $\vec u, \vec v \in W$, we have $\vec u + \vec v \in W$.
        \item (Closure under multiplication) For all $c \in \RR$ and $\vec u \in W$, we have $c \vec u \in W$.
    \end{itemize}
\end{recipe}

Conversely, to show that $W$ is not a subspace, we can try to disprove any of the three conditions. Typically, the first condition ($\vec 0 \in W$) is the easiest to disprove. If that fails, we construct a counter-example for closure under addition/multiplication.

\begin{sample}\label{sample:linear-transformations:planes}
    Let $W$ be any plane in $\RR^3$ that passes through the origin. Prove that $W$ is a subspace of $\RR^3$ under the standard operations.
\end{sample}
\begin{sampans}
    Let $W = \bc{\vec r = \l \vec a + \m \vec b \mid \l, \m \in \RR}$.
    \begin{itemize}
        \item Taking $\l = \m = 0$, we see that $\vec 0 \in W$.
        \item Take any two elements of $W$, say $\vec r_1 = \l_1 \vec a + \m_1 \vec b$ and $\vec r_2 = \l_2 \vec a + \m_2 \vec b$. Their sum is \[\vec r_1 + \vec r_2 = \bp{\l_1 \vec a + \m_1 \vec b} + \bp{\l_2 \vec a + \m_2 \vec b} = \bp{\l_1 + \l_2} \vec a + \bp{\m_1 + \m_2} \vec b.\] Since $\l_1 + \l_2, \m_1 + \m_2 \in \RR$, it follows that $\vec r_1 + \vec r_2 \in W$, so $W$ is closed under addition.
        \item Take an arbitrary element of $W$, say $\vec r = \l \vec a + \m \vec b$, and let $k \in \RR$ be a scalar. Then \[k\vec r = k\bp{\l \vec a + \m \vec b} = \bp{k \l} \vec a + \bp{k \m} \vec b.\] Since $k\l, k\m \in \RR$, it follows that $k \vec r \in W$, so $W$ is closed under multiplication.
    \end{itemize}
    Thus, $W$ is a subspace of $\RR^3$.
\end{sampans}

\begin{sample}
    Let $W$ be the set of vectors in $\RR^3$ whose length does not exceed 1. Determine whether $W$ is a subspace of $\RR^3$.
\end{sample}
\begin{sampans}
    Take $\vec u = \cveciiix100$ and $\vec v = \cveciiix010$. Since $\abs{\vec u} = \abs{\vec v} = 1 \leq 1$, they are both elements of $W$. Now consider the length of $\vec u + \vec v$: \[\abs{\vec u + \vec v} = \abs{\cveciii100 + \cveciii010} = \abs{\cveciii110} = \sqrt{2} \geq 1.\] Thus, $\vec u + \vec v \notin W$, so $W$ is not closed under addition. Thus, $W$ is not a linear space, so $W$ is not a subspace of $\RR^3$.
\end{sampans}

In Sample Problem~\ref{sample:linear-transformations:planes}, we saw how any plane passing through the origin in $\RR^3$ is a subspace. We can generalize this further:

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    \multicolumn{1}{|c|}{\bd{Subspaces of $\RR^1$}} & \multicolumn{1}{c|}{\bd{Subspaces of $\RR^2$}}
     & \multicolumn{1}{c|}{\bd{Subspaces of $\RR^3$}} \\ \hline\hline
    \tabitem $\bc{\vec 0}$ & \tabitem $\bc{\vec 0}$ & \tabitem $\bc{\vec 0}$ \\
    \tabitem $\RR^1$ & \tabitem Lines through the origin & \tabitem Lines through the origin \\ 
     & \tabitem $\RR^2$ & \tabitem Planes through the origin \\ 
     &  & \tabitem $\RR^3$ \\ \hline
    \end{tabular}
\end{table}

In fact, these are the only subspaces of $\RR^1$, $\RR^2$ and $\RR^3$. Note that this pattern holds for all $\RR^n$.

\section{Span and Linear Independence}

\subsection{Linear Spans}

\begin{definition}
    Let $S = \bc{\vec v_1, \dots, \vec v_r}$ be a non-empty subset of a linear space $V$. Then the \vocab{span} of $S$, denoted $\Span S$ or $\Span{\vec v_1, \dots, \vec v_r}$, is the set of all linear combinations of vectors of $S$. That is, $\Span S = \bc{a_1 \vec v_1 + \dots + a_r \vec v_r \mid a_1, \dots, a_r \in \RR}$.
\end{definition}

Note that $\Span \varnothing = \bc{\vec 0}$, since the empty sum is $\vec 0$.

\begin{proposition}
    Let $S$ be a subset of a linear space $V$. Then $\Span S$ is a subspace of $V$.
\end{proposition}
\begin{proof}
    Let $S = \bc{\vec v_1, \dots, \vec v_r}$, so $\Span S = \bc{a_1 \vec v_1 + \dots + a_r \vec v_r \mid a_1, \dots, a_r \in \RR}$.
    \begin{itemize}
        \item Taking $a_1 = \dots = a_n = 0$, we see that $\vec 0 \in \Span S$.
        \item Take two arbitrary elements of $\Span S$, say $\vec a = a_1 \vec v_1 + \dots + a_r \vec v_r$ and $\vec b = b_1 \vec v_1 + \dots + b_r \vec v_r$. Their sum is
        \begin{align*}
            \vec a + \vec b &= \bp{a_1 \vec v_1 + \dots + a_r \vec v_r} + \bp{b_1 \vec v_1 + \dots + b_r \vec v_r}\\
            &= \bp{a_1 + b_1} \vec v_1 + \dots + \bp{a_r + b_r} \vec v_r.
        \end{align*}
        Since $a_1 + b_1, \dots, a_r + b_r \in \RR$, it follows that $\vec a + \vec b \in \Span S$, so $\Span S$ is closed under addition.
        \item Take an arbitrary element of $\Span S$, say $\vec a = a_1 \vec v_1 + \dots + a_r \vec v_r$, and let $k \in \RR$ be a scalar. Then \[k \vec a = k \bp{a_1 \vec v_1 + \dots + a_r \vec v_r} = ka_1 \vec v_1 + \dots + ka_r \vec v_r.\] Since $ka_1, \dots, ka_r \in \RR$, it follows that $k\vec a \in \Span S$, so $\Span S$ is closed under scalar multiplication.
    \end{itemize}
    Thus, $S$ is a subspace of $V$.
\end{proof}

To determine if a given vector is in the span of a set of vectors, we can apply the following result.

\begin{proposition}\label{prop:lin-alg-col-span}
    The vector $\vec b$ is in the span of $\bc{\vec v_1, \dots, \vec v_n}$ if and only if the matrix equation $\mat A \vec x = \vec b$ has a solution, where $\mat A = \begin{pmatrix}\vec v_1 & \dots & \vec v_n\end{pmatrix}$.
\end{proposition}
\begin{proof}
    $\vec x = \cveciiix{x_1}{\dots}{x_n}$ is a solution to the matrix equation $\mat A \vec x = \vec b$ if and only if \[\vec b = \mat A \vec x = \begin{pmatrix}\vec v_1 & \dots & \vec v_n \end{pmatrix} \cveciii{x_1}{\vdots}{x_n} = x_1 \vec v_1 + \dots + x_n \vec v_n \in \Span{\vec v_1, \dots, \vec v_n}.\]
\end{proof}

\begin{sample}
    Determine if $\RR^3$ is spanned by \[S = \bc{\cveciii121, \cveciii102}.\]
\end{sample}
\begin{sampans}
    Consider the matrix \[\mat A = \begin{pmatrix}1 & 1 \\ 2 & 0 \\ 1 & 2\end{pmatrix}\] and let $\vec b = \cveciiix{a}{b}{c}$ be an arbitrary vector in $\RR^3$. Consider the matrix equation $\mat A \vec x = \vec b$, or equivalently, the augmented matrix \[\begin{pmatrix}[cc|c]1 & 1 & a \\ 2 & 0 & b\\ 1 & 2 & c\end{pmatrix}.\] Performing row-operations, we obtain \[\begin{pmatrix}[cc|c]1 & 0 & b/2 \\ 0 & 1 & c-a \\ 0 & 0 & 2a - b/2 - c\end{pmatrix}.\] The system is only consistent when $2a - b/2 - c = 0$. That is, not all vectors in $\RR^3$ can be written as a linear combination of vectors in $S$. Thus, $\RR^3$ is not spanned by $S$.
\end{sampans}

\begin{sample}
    Determine if $\RR^3$ is spanned by \[S = \bc{\cveciii121, \cveciii102, \cveciii110, \cveciii100}.\]
\end{sample}
\begin{sampans}
    Consider the equation \[x_1 \cveciii121 + x_2 \cveciii102 + x_3 \cveciii110 + x_4 \cveciii100 = \cveciii{a}{b}{c}.\] We may rewrite this as \[\begin{pmatrix}1 & 1 & 1 \\ 2 & 0 & 1 \\ 1 & 2 & 0\end{pmatrix} \cveciii{x_1}{x_2}{x_3} = \cveciii{a-x_4}{b}{c}.\] Since the matrix on the LHS has non-zero determinant, it is invertible, so there exist $x_1, x_2, x_3, x_4 \in \RR$ satisfying the above equation is satisfied. That is to say, every vector in $\RR^3$ can be expressed as a linear combination of vectors in $S$. Thus, $\RR^3$ is spanned by $S$.
\end{sampans}

\subsection{Linear Independence}

Consider the previous sample problem. For different choices of $x_4$, we get different values of $x_1, x_2, x_3$. That is, for a particular vector $\vec v$, there is more than one way of expressing $\vec v$ as a linear combination of the vectors in $S$. This is because the fourth vector, $\cveciiix100$, is a linear combination of the other three vectors and is hence redundant. In particular, \[\cveciii100 = -\frac23 \cveciii121 + \frac13 \cveciii102 + \frac43 \cveciii110.\] We say that $S$ is linearly dependent.

\begin{definition}
    A set of vectors $\bc{\vec v_1, \dots, \vec v_k}$ is \vocab{linearly independent} if the only solution to $x_1 \vec v_1 + \dots + x_k \vec v_k = \vec 0$ is the trivial solution $x_1 = \dots = x_k = 0$. If there are non-trivial solutions, we say that the set of vectors is \vocab{linearly dependent}.
\end{definition}

Equivalently, the set of vectors are linearly dependent if at least one vector is expressible as a linear combination of the other vectors.

\begin{proposition}\label{prop:lin-alg-lin-dep-inv}
    Let $\mat A$ be invertible. Then $\bc{\vec v_1, \dots, \vec v_n}$ is linearly dependent if and only if $\bc{\mat A \vec v_1, \dots, \mat A \vec v_n}$.
\end{proposition}
\begin{proof}
    ($\implies$) Suppose that $\bc{\vec v_1, \dots, \vec v_n}$ is linearly dependent. Then there exist scalars $x_1, \dots, x_n$, not all zero, such that $x_1 \vec v_1 + \dots + x_n \vec v_n = \vec 0$. Pre-multiplying by $\mat A$ on both sides, we get $x_1 \bp{\mat A \vec v_1} + \dots + x_n \bp{\mat A \vec v_n} = \vec 0$. Thus, by definition, the set $\bc{\mat A \vec v_1, \dots, \mat A \vec v_n}$ is linearly dependent.

    ($\impliedby$) Suppose that $\bc{\mat A \vec v_1, \dots, \mat A \vec v_n}$ is linearly dependent. Then there exist scalars $x_1, \dots, x_n$, not all zero, such that $x_1 \bp{\mat A \vec v_1} + \dots + x_n \bp{\mat A \vec v_n} = \vec 0$. Since $\mat A$ is invertible, we may pre-multiply by $\mat A^{-1}$ on both sides to get $x_1 \vec v_1 + \dots + x_n \vec v_n = \vec 0$. Thus, by definition, the set $\bc{\vec v_1, \dots, \vec v_n}$ is linearly dependent.
\end{proof}

We can use the following strategy to test if a set of vectors is linearly independent.

\begin{recipe}[Test for Linear Independence]
    We are given $r$ vectors $\vec v_1, \dots, \vec v_r \in \RR^n$.

    \case{1} If $r > n$, then the $r$ vectors must be linearly dependent.
    
    \case{2} If $r \leq n$, we find $\vec x = \cveciiix{x_1}{\dots}{x_r}$ such that $\mat A \vec x = \vec 0$ where $\mat A = \begin{pmatrix}\vec v_1 & \dots & \vec v_r\end{pmatrix}$ is an $n \times r$ matrix. Whether the $r$ vectors are linearly dependent becomes a question of whether the equation $\mat A \vec x = \vec 0$ has only the trivial solution $\vec x = \vec 0$. To answer this question we can
    \begin{itemize}
        \item (in general) use row operations to reduce $\mat A$ to REF. If there are exactly $r$ non-zero rows, then $\mat A \vec x = \vec 0$ has only the trivial solution.
        \item (if $r = n$) compute the determinant of $\mat A$. If $\det \mat A \neq 0$, then $\mat A \vec x = \vec 0$ has only the trivial solution.
    \end{itemize}
\end{recipe}

\begin{sample}
    Determine if the following set of vectors is linearly independent: \[S = \bc{\cveciii121, \cveciii102}.\]
\end{sample}
\begin{sampans}
    Consider the matrix equation \[\begin{pmatrix}1 & 1 \\ 2 & 0 \\ 1 & 2 \end{pmatrix} \cvecii{x_1}{x_2} = \cveciii000.\] Reducing the matrix on the left to its RREF, we obtain \[\begin{pmatrix}1 & 0 \\ 0 & 1 \\ 0 & 0 \end{pmatrix} \cvecii{x_1}{x_2} = \cveciii000.\] Thus, the only solutions are $x_1 = x_2 = 0$, so $S$ is linearly independent.
\end{sampans}

\begin{sample}
    Determine if the following set of vectors is linearly independent: \[S = \bc{\cveciii121, \cveciii102, \cveciii110, \cveciii100}.\]
\end{sample}
\begin{sampans}
    $S$ is a set of 4 vectors in $\RR^3$. Since $4 > 3$, it follows that $S$ is linearly dependent.
\end{sampans}

\subsubsection{Geometrical Interpretations of Linear Independence}

In $\RR^2$, two vectors $\vec u$ and $\vec v$ are linearly dependent if and only if they lie on the same line (with their initial points at the origin).

\begin{minipage}{0.5 \textwidth}
    \begin{figure}[H]\tikzsetnextfilename{449}
    \centering
    \begin{tikzpicture}[scale=2]
        \draw[black, thick, ->] (0, 0) -- (1, 1) node[anchor=north west] {$\vec u$};
        \draw[black, thick, ->] (0, 0) -- (0, 1) node[anchor=north east] {$\vec v$};
        \node[anchor=east] (0, 0) {$O$};
    \end{tikzpicture}
    \caption{Linearly independent vectors.}
    \end{figure}
\end{minipage}
\begin{minipage}{0.5 \textwidth}
    \begin{figure}[H]\tikzsetnextfilename{450}
    \centering
    \begin{tikzpicture}[scale=2]
        \draw[black, thick, ->] (0, 0) -- (1, 1) node[anchor=north west] {$\vec u$};
        \draw[black, thick, ->] (0, 0) -- (0.5, 0.5) node[anchor=south east] {$\vec v$};
        \node[anchor=east] (0, 0) {$O$};
    \end{tikzpicture}
    \caption{Linearly dependent vectors.}
    \end{figure}
\end{minipage}
In $\RR^3$, three vectors $\vec u$, $\vec v$ and $\vec w$ are linearly dependent if and only if they lie on the same line or plane (with their initial points at the origin).

\section{Basis and Dimension}

\begin{definition}
    A \vocab{basis} $S = \bc{\vec v_1, \dots, \vec v_r}$ for a linear space $V$ is a set of vectors such that
    \begin{itemize}
        \item $S$ spans $V$, and
        \item $S$ is linearly independent.
    \end{itemize}
\end{definition}

\begin{definition}
    Let $\vec e_1 = (1, 0, \dots, 0)\trp$, $\vec e_2 = (0, 1, 0, \dots, 0)\trp$, $\dots$, $\vec e_n = (0, \dots, 0, 1)\trp$. The set $\bc{\vec e_1, \vec e_2, \dots, \vec e_n}$ is called the \vocab{standard basis} of $\RR^n$.
\end{definition}

\begin{sample}
    Show that the set \[S = \bc{\cveciv1010, \cveciv01{-1}2, \cveciv0221, \cveciv1001}\] is a basis for $\RR^4$.
\end{sample}
\begin{sampans}
    We first show that $S$ spans $\RR^4$. Consider $\vec v = \cvecivx{a}{b}{c}{d}$, where $a, b, c, d \in \RR$. Consider the matrix equation \[\begin{pmatrix}1 & 0 & 0 & 1\\ 0 & 1 & 2 & 0 \\ 1 & -1 & 2 & 0 \\ 0 & 2 & 1 & 1\end{pmatrix}\cveciv{k_1}{k_2}{k_3}{k_4} = \cveciv{a}{b}{c}{d}.\] Since the matrix on the LHS has non-zero determinant, every $\vec v$ can be expressed as a linear combination of the vectors of $S$. Thus, $S$ spans $\RR^4$.

    We now show that $S$ is linearly independent. Consider the case where $\vec v = \vec 0$. \[\begin{pmatrix}1 & 0 & 0 & 1\\ 0 & 1 & 2 & 0 \\ 1 & -1 & 2 & 0 \\ 0 & 2 & 1 & 1\end{pmatrix}\cveciv{k_1}{k_2}{k_3}{k_4} = \cveciv0000.\] Since the matrix on the LHS has non-zero determinant, the equation has only the trivial solution. Thus, $S$ is linearly independent.
\end{sampans}

One particularly useful property about bases is that there is only one way to build a vector as a linear combination of given basis vector.

\begin{theorem}
    If $\bc{\vec v_1, \dots, \vec v_n}$ is a basis for a linear space $V$, then every vector $\vec v \in V$ can be expressed in the form $\vec v = k_1 \vec v_1 + \dots + k_n \vec v_n$ in exactly one way.
\end{theorem}

While a linear space can have many different bases, the number of basis vectors forming each basis must be the same. This number is called the dimension of $V$.

\begin{definition}
    The \vocab{dimension} of a non-zero linear space $V$ is the number of vectors in a basis for $V$, and is denoted $\Dim V$. By convention, we define the dimension of the zero linear space $\bc{\vec 0}$ to be 0.
\end{definition}

As an example, the linear space $\RR^n$ has dimension $n$ (recall that the standard basis consists of $n$ vectors).

We now state several results relating spans, linear independence and bases.

\begin{proposition}
    Let $V$ be a linear space with finite dimension $n$, and let $S \subseteq V$.
    \begin{itemize}
        \item If $\abs{S} > n$, then $S$ is linearly dependent.
        \item If $\abs{S} < n$, then $S$ cannot span $V$.
        \item If $\abs{S} = n$, then $S$ is a basis of $V$ if and only if $S$ is linearly independent if and only if $S$ spans $V$.
    \end{itemize}
\end{proposition}

The last property allows us to easily determine if a set is a basis of a linear space.

\begin{proposition}
    Let $V$ be a linear space with finite dimension $n$, and let $S \subseteq V$ be finite.
    \begin{itemize}
        \item If $S$ spans $V$ but is not a basis of $V$, then it can be reduced to a basis by removing certain vectors from $S$.
        \item If $S$ is linearly independent but not a basis of $V$, then it can be enlarged to a basis by adding in certain vectors from $V$.
    \end{itemize}
\end{proposition}

\section{Vector Spaces Associated with Matrices}

\subsection{Row Space, Column Space and Null Space}

Given an $m \times n$ matrix, there are three special subspaces of $\RR^m$ and $\RR^n$, namely the row space, column space and null space.

\begin{definition}
    Let $\mat A = (a)_{ij}$ be an $m \times n$ matrix. Define the \vocab{row vectors} of $\mat A$ to be \[\vec r_i = \begin{pmatrix}a_{i1} & a_{i2} & \dots & a_{in}\end{pmatrix}\trp.\] Then the \vocab{row space} of $\mat A$, denoted $\Row \mat A$, is the span of the row vectors of $\mat A$.
\end{definition}

Because it is the span of vectors in $\RR^n$, it is a subspace of $\RR^n$.

\begin{definition}
    Let $\mat A = (a)_{ij}$ be an $m \times n$ matrix. Define the \vocab{column vectors} of $\mat A$ to be \[\vec c_j = \begin{pmatrix}a_1j \\ a_2j \\ \vdots \\ a_mj \end{pmatrix}.\] Then the \vocab{column space} of $\mat A$, denoted $\Col \mat A$, is the span of the column vectors of $\mat A$.
\end{definition}

Because it is the span of vectors in $\RR^m$, it is a subspace of $\RR^m$.

\begin{definition}
    Let $\mat A$ be an $m \times n$ matrix. The \vocab{null space} of $\mat A$ is the solution set to the homogeneous system of equations $\mat A \vec x = \vec 0$, i.e. $\bc{\vec x \in \RR^n : \mat A \vec x = \vec 0}$.
\end{definition}

The null space is a subspace of $\RR^n$.

\begin{proposition}
    The row space is orthogonal to the null space.
\end{proposition}
\begin{proof}
    Let $\vec x$ be in the null space of $\mat A$, and let $\vec y$ be in the row space of $\mat A$. Let $\vec r_i$ be the $i$th row vector of $\mat A$. Then \[\mat A \vec x = \cveciii{\vec r_1 \dotp \vec x}{\vdots}{\vec r_m \dotp \vec x} = \cveciii0{\vdots}0.\] It follows that $\vec r_i \dotp \vec x = 0$ for all $1 \leq i \leq m$. Thus, \[\vec y \dotp \vec x = \bp{\sum_{i = 1}^m k_i \vec r_i} \dotp \vec x = \sum_{i = 1}^m k_i \bp{\vec r_i \dotp \vec x} = 0,\] so $\vec y$ and $\vec x$ are orthogonal. Thus, the row space is orthogonal to the null space.
\end{proof}

\subsection{Range Space and Kernel}

Let the linear transformation $T : \RR^n \to \RR^m$ be represented by the $m \times n$ matrix $\mat A$. In this section, we will introduce two special subspaces related to $T$, namely the range space and kernel of $T$. These two subspaces are equal to the column and null spaces of $\mat A$ respectively.

\begin{definition}
    The \vocab{range space} of $T$, denoted $\Range T$, consists of all vectors $\vec b$ such that $\mat A \vec x = \vec b$.
\end{definition}

\begin{proposition}
    $\Range T$ is equal to the column space of $\mat A$.
\end{proposition}
\begin{proof}
    Follows immediately from Proposition~\ref{prop:lin-alg-col-span}.
\end{proof}

\begin{definition}
    The \vocab{kernel} of $T$, denoted $\Ker T$, is the set of all vectors $\vec x$ such that $\mat A \vec x = 0$.
\end{definition}

\begin{proposition}
    $\Ker T$ is equal to the null space of $\mat A$.
\end{proposition}
\begin{proof}
    Trivial.
\end{proof}

\subsection{Basis for Row Space}

\begin{definition}
    Two matrices $\mat A$ and $\mat B$ are said to be \vocab{row-equivalent} if their row spaces are the same.
\end{definition}

\begin{proposition}
    $\mat A$ and its REF/RREF are row-equivalent.
\end{proposition}
\begin{proof}
    Recall that an elementary row operation produces a new row that is a linear combination of the old rows. Thus, elementary row operations do not change the row space of a matrix. Since the REF/RREF of $\mat A$ can be obtained solely from elementary row operations, it follows that $\mat A$ and its REF/RREF are row-equivalent.
\end{proof}

This result allows us to easily find the basis of the row space of $\mat A$.

\begin{recipe}[Finding Basis of Row Space]
    Let $\mat B$ be the REF/RREF of $\mat A$. Then the non-zero row vectors in $\mat B$ form a basis for the row space of $\mat A$.
\end{recipe}

\begin{sample}
    Find a basis for the row space of \[\mat A = \begin{pmatrix}1 & 2 & 3 & 4 & 5 \\ 6 & 7 & 8 & 9 & 10 \\ 11 & 12 & 13 & 14 & 15 \\ 16 & 17 & 18 & 19 & 21\end{pmatrix}.\]
\end{sample}
\begin{sampans}
    The RREF of $\mat A$ is given by \[\begin{pmatrix}1 & 0 & -1 & -2 & 0 \\ 0 & 1 & 2 & 3 & 0 \\ 0 & 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0 & 0\end{pmatrix}.\] Thus, a row space basis of $\mat A$ is \[\bc{\begin{pmatrix}1 \\ 0 \\ -1 \\ -2 \\ 0\end{pmatrix}, \begin{pmatrix}0 \\ 1 \\ 2 \\ 3 \\ 0\end{pmatrix}, \begin{pmatrix}0 \\ 0 \\ 0 \\ 0 \\ 1\end{pmatrix}}.\]    
\end{sampans}

\subsection{Basis for Column Space}

One way of finding a basis for the column space of $\mat A$ would be to find a basis for the row space of $\mat A \trp$. However, there is a much simpler approach, which we now derive.

\begin{proposition}\label{prop:lin-alg:row-lindep}
    Row operations do not change the linear dependence on columns.
\end{proposition}
\begin{proof}
    Recall that a sequence of elementary row-operations can be represented by an invertible matrix $\mat E$. The desired claim immediately follows by Proposition~\ref{prop:lin-alg-lin-dep-inv}.
\end{proof}

Row operations, however, do not preserve the column space of $\mat A$. For instance, \[\begin{pmatrix}1 & 0 \\ 0 & 0\end{pmatrix} \quad \tand \quad \begin{pmatrix}0 & 0 \\ 1 & 0\end{pmatrix}\] are row-equivalent, but their column spaces are entirely different.

As a consequence of the above result, we obtain the following corollaries:
\begin{corollary}
    If $\mat A$ and $\mat B$ are row-equivalent, a given set of columns of $\mat A$ forms a basis for $\Col{\mat A}$ if and only if the corresponding set of columns of $\mat B$ forms a basis for $\Col{\mat B}$.
\end{corollary}

With this, we have our standard procedure for finding a basis for the column space of $\mat A$:

\begin{recipe}[Finding Basis of Column Space]
    Let $\mat B$ be the REF/RREF of $\mat A$. Look at the columns of $\mat B$ with a leading entry. Then the corresponding columns of $\mat A$ form a basis of $\Col{\mat A}$.
\end{recipe}

\begin{sample}
    Find a basis for the column space of \[\mat A = \begin{pmatrix}1 & 2 & 3 & 4 & 5 \\ 6 & 7 & 8 & 9 & 10 \\ 11 & 12 & 13 & 14 & 15 \\ 16 & 17 & 18 & 19 & 21\end{pmatrix}.\]
\end{sample}
\begin{sampans}
    The RREF of $\mat A$ is given by \[\mat B = \begin{pmatrix}\boxed{1} & 0 & -1 & -2 & 0 \\ 0 & \boxed{1} & 2 & 3 & 0 \\ 0 & 0 & 0 & 0 & \boxed{1} \\ 0 & 0 & 0 & 0 & 0\end{pmatrix}.\] The first, second and fifth columns of $\mat B$ contain a leading entry. Thus, the first, second and fifth columns of $\mat A$ form a basis of $\Col{\mat A}$: \[\bc{\cveciv16{11}{16}, \cveciv27{12}{17}, \cveciv5{10}{15}{21}}.\]
\end{sampans}

\subsection{Basis for Null Space}

By Proposition~\ref{prop:lin-alg:row-lindep}, if $\mat B$ is the REF/RREF of $\mat A$, then the equations $\mat A \vec x = 0$ and $\mat B \vec x = \vec 0$ will have the same solution set. 

\begin{recipe}[Finding Basis of Null Space]
    Let $\mat B$ be the REF/RREF of $\mat A$. Then the null space of $\mat A$ is the solution set $\vec x$ of $\mat B \vec x = \vec 0$.
\end{recipe}

\begin{sample}
    Find a basis for the null space of \[\mat A = \begin{pmatrix}1 & 2 & 3 & 4 & 5 \\ 6 & 7 & 8 & 9 & 10 \\ 11 & 12 & 13 & 14 & 15 \\ 16 & 17 & 18 & 19 & 21\end{pmatrix}.\]
\end{sample}
\begin{sampans}
    The RREF of $\mat A$ is \[\mat B = \begin{pmatrix}1 & 0 & -1 & -2 & 0 \\ 0 & 1 & 2 & 3 & 0 \\ 0 & 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0 & 0\end{pmatrix}.\] The matrix equation $\mat B \vec x = \vec 0$ gives the system of linear equations \[\systeme{x_1 - x_3 - 2x_4 = 0, x_2 + 2x_3 + 3x_4 = 0, x_5 = 0}.\] Setting $x_3 = s$ and $x_4 = t$, we have \[\begin{pmatrix}x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \end{pmatrix} = \begin{pmatrix}s + 2t \\ -2s - 3t \\ s \\ t \\ 0 \end{pmatrix} = s \begin{pmatrix}1 \\ -2 \\ 1 \\ 0 \\ 0\end{pmatrix} + t \begin{pmatrix}2 \\ -3 \\ 0 \\ 1 \\ 0 \end{pmatrix}.\] Thus, the basis of the null space of $\mat A$ is \[\bc{\begin{pmatrix}1 \\ -2 \\ 1 \\ 0 \\ 0\end{pmatrix}, \begin{pmatrix}2 \\ -3 \\ 0 \\ 1 \\ 0 \end{pmatrix}}.\]
\end{sampans}

\section{Rank and Nullity for Matrices}

\begin{definition}
    The \vocab{row rank} of $\mat A$ is the dimension of the row space of $\mat A$. The \vocab{column rank} of $\mat A$ is the dimension of the column space of $\mat A$.
\end{definition}

\begin{proposition}
    Row and column ranks are equal.
\end{proposition}
\begin{proof}
    Recall the procedure we took to find the basis for the row and column space of a matrix:
    \begin{itemize}
        \item The column space basis consists of columns in the original matrix corresponding to the leading entries in the REF/RREF.
        \item The row space basis consists of the rows of the REF/RREF corresponding to the leading entries.
    \end{itemize}
    Since each leading entry corresponds to exactly one row and one column, the sizes of the row and column spaces bases must be equal. Hence, the row and column ranks are equal.
\end{proof}

We give this common value a special name.

\begin{definition}
    The \vocab{rank} of $\mat A$ is the dimension of the row/column space of $\mat A$. It is denoted by $\Rank \mat A$.
\end{definition}

Let $\mat A$ be an $m \times n$ matrix. Because the row rank is at most $m$, and the column rank is at most $n$, we have that $\Rank \mat A \leq \min{m, n}$. If equality is achieved, we give $\mat A$ a special name:

\begin{definition}
    Let $\mat A$ be an $m \times n$ matrix. If $\Rank \mat A = \min{m, n}$, we say $\mat A$ has \vocab{full rank}.
\end{definition}

\begin{proposition}
    $\Rank{\mat A \mat B} \leq \min{\Rank \mat A, \Rank \mat B}$.
\end{proposition}
\begin{proof}
    Every column in $\mat A \mat B$ can be expressed as a linear combination of the columns of $\mat A$, so $\Col{\mat A \mat B} \subseteq \Col \mat A$. Taking dimensions, we see that \[\Rank{\mat A \mat B} = \dim \Col{\mat A \mat B} \leq \dim \Col \mat A = \Rank \mat A.\] Similarly, every row in $\mat A \mat B$ can be expressed as a linear combination of the rows of $\mat B$, so $\Row{\mat A \mat B} \subseteq \Row \mat B$. Taking dimensions, \[\Rank{\mat A \mat B} = \dim \Row{\mat A \mat B} \leq \dim \Row \mat B = \Rank \mat B.\] Combining these two inequalities gives us what we want.
\end{proof}

We can slightly extend the above result:

\begin{proposition}
    For any $n \times n$ matrices $\mat A$ and $\mat B$, where $\mat B$ is invertible, we have $\Rank{\mat A \mat B} = \Rank{\mat B \mat A} = \Rank \mat A$.
\end{proposition}
\begin{proof}
    Observe that \[\Rank \mat A = \Rank{\mat A \mat B \mat B^{-1}} \leq \Rank{\mat A \mat B} \leq \Rank \mat A,\] so $\Rank{\mat A \mat B} = \Rank \mat A$. Similarly, \[\Rank \mat A = \Rank{\mat B^{-1} \mat B \mat A} \leq \Rank{\mat B \mat A} \leq \Rank \mat A,\] so $\Rank{\mat B \mat A} = \Rank \mat A$.
\end{proof}

\begin{definition}
    The \vocab{nullity} of $\mat A$ is the dimension of the null space of $\mat A$. It is denoted by $\Nullity \mat A$.
\end{definition}

\begin{theorem}[Rank-Nullity Theorem]
    For an $m \times n$ matrix $\mat A$, \[\Rank \mat A + \Nullity \mat A = \text{$n$, the number of columns of $\mat A$}.\]
\end{theorem}
\begin{proof}
    $\Rank \mat A$ is equal to the number of columns in the RREF that contains a leading entry, while $\Nullity \mat A$ is equal to the number of columns in the RREF that does not contain a leading entry. Thus, their sum must be the number of columns in the RREF, which is $n$.
\end{proof}

We can determine the number of solutions to a system of linear equations using the rank of its corresponding matrix:

\begin{recipe}[Finding Number of Solutions]
    Let $\mat A \vec x = \vec b$ be a system of linear equations in $n$ variables.
    \begin{itemize}
        \item If $\Rank \mat A = \Rank \begin{pmatrix}[c|c] \mat A & \vec b\end{pmatrix} = n$, the system if consistent and has a unique solution.
        \item If $\Rank \mat A = \Rank \begin{pmatrix}[c|c] \mat A & \vec b\end{pmatrix} < n$, then the system is consistent and has an infinite number of solutions.
        \item If $\Rank \mat A < \Rank \begin{pmatrix}[c|c] \mat A & \vec b\end{pmatrix}$, then the system is inconsistent and thus has no solution.
    \end{itemize}
\end{recipe}

In the case where the system is consistent, we can apply the following result to find all possible solutions to the system:

\begin{proposition}\label{prop:lin-alg-solution-space}
    Let $\vec x_p$ be a particular solution of $\mat A \vec x = \vec b$, and let $K = \bc{\vec x_h \mid \mat A \vec x_h = \vec 0}$ be the null space of $\mat A$. Then the set of solutions to $\mat A \vec x = \vec b$ is $S = \bc{\vec x_p + \vec x_h \mid \vec x_h \in K}$.
\end{proposition}
We will prove the equivalent statement $\mat A \vec x = \vec b \iff \vec x \in S$.
\begin{proof}
    ($\implies$) Suppose $\mat A \vec x = \vec b$. Since $\vec x_p$ is a particular solution, we know that $\mat A \vec x_p = \vec b$. Thus, \[\mat A \bp{\vec x - \vec x_p} = \mat A \vec x - \mat A \vec x_p = \vec b - \vec b = \vec 0,\] so $\vec x - \vec x_p$ is in the null space $K$. Taking $\vec x_h = \vec x - \vec x_p$, we immediately see that $\vec x \in S$.

    ($\impliedby$) Suppose $\vec x \in S$. That is, there exists some $\vec x_h \in K$ such that $\vec x = \vec x_p + \vec x_h$. Applying $\mat A$ on both sides of the equation, we get \[\mat A \vec x = \mat A \bp{\vec x_p + \vec x_h} = \mat A \vec x_p + \mat A \vec x_h = \vec b + \vec 0 = \vec b.\]
\end{proof}

\section{Rank and Nullity for Linear Transformations}

\begin{definition}
    Let $T$ be a linear transformation. The dimension of the range of $T$ is called the \vocab{rank} of $T$ and the dimension of the kernel of $T$ is called the \vocab{nullity} of $T$.
\end{definition}

\begin{theorem}[Rank-Nullity Theorem for Linear Transformations]
    For a linear transformation $T : \RR^m \to \RR^n$, where $T(\vec x) = \mat A \vec x$, we have \[\Rank T + \Nullity T = \Rank \mat A + \Nullity \mat A = n.\]
\end{theorem}
\begin{proof}
    Recall that the range of $T$ is the column space of $\mat A$ and the kernel of $T$ is the null space of $\mat A$. Hence, \[\Rank T = \Dim \Range T = \Dim \Col \mat A = \Rank \mat A\] and \[\Nullity T = \Dim \Ker T = \Dim{\text{null space of $\mat A$}} = \Nullity \mat A.\] By the Rank-Nullity Theorem for matrices, we have \[\Rank T + \Nullity T = \Rank \mat A + \Nullity \mat A = n.\]
\end{proof}